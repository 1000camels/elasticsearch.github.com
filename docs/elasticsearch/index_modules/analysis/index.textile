---
layout: doc_es
title: ElasticSearch Docs | Index Modules | Analysis
---

<script type="text/javascript">
docBreadcrumb = [
    ["elasticsearch", "ElasticSearch"], 
    ["index_modules", "Index Modules"], 
    ["analysis", "Analysis"], 
];
</script>

h1. Index Analysis Module

p. The index analysis module acts as a configurable registry of @Analyzers@ that can be used in order to both break indexed (analyzed) fields when a document is indexed and process query strings. It maps to Lucene @Analyzer@. 

p. Analyzers in general are broken down into a @Tokenizer@ with zero or more @TokenFilter@ applied to it. The analysis module allows to register @TokenFilters@, @Tokenizers@ and @Analyzers@ under logical names which can then be referenced either in mapping definitions or in certain APIs. The Analysis module automatically registers (*if not explicitly defined*) built in analyzers, token filters, and tokenizers. 

p. Here is a sample configuration:

<pre class="prettyprint">
index :
    analysis :
        analyzer : 
            standard : 
                type : standard
                stopwords : [stop1, stop2]
            myAnalyzer1 :
                type : standard
                stopwords : [stop1, stop2, stop3]
                max_token_length : 500
            # configure a custom analyzer which is 
            # exactly like the default standard analyzer
            myAnalyzer2 :
                tokenizer : standard
                filter : [standard, lowercase, stop]
        tokenizer :
            myTokenizer1 :
                type : standard
                max_token_length : 900
            myTokenizer2 :
                type : keyword
                buffer_size : 512
        filter :
            myTokenFilter1 :
                type : stop
                stopwords : [stop1, stop2, stop3, stop4]
            myTokenFilter2 :
                type : length
                min : 0
                max : 2000
</pre>

h2. Built in Analyzers

p. If not explicitly defined (for example, by configuring an analyzer with the same logical name), the following analyzers are automatically registered (under their respective logical names) and available for use:

|_. Analyzer Logical Name |_. Description |
|standard|A "Standard Analyzer":#Standard_Analyzer registered with default settings.|
|simple|A "Simple Analyzer":#Simple_Analyzer registered with default settings.|
|stop|A "Stop Analyzer":#Stop_Analyzer registered with default settings.|
|whitespace|A "Whitespace Analyzer":#Whitespace_Analyzer registered with default settings.|
|keyword|A "Keyword Analyzer":#Keyword_Analyzer registered with default settings.|

h2. Default Analyzers

p. An analyzer is registered under a logical name and can then be referenced from mapping definitions or certain APIs. When none are defined, defaults are used. There is an option to define which analyzers will be used as default when none can be derived.

p. The @default@ logical name allows to configure an analyzer that will be used both for indexing and for searching APIs. The @default_index@ logical name can be used to configure a default analyzer that will be used just when indexing, and the @default_search@ can be used to configure a default analyzer that will be used just when indexing.

h2. Standard Analyzer

p. An analyzer of type @standard@ that is built of using "Standard Tokenizer":#Standard_Tokenizer, with "Standard Token Filter":#Standard_Token_Filter, "Lower Case Token Filter":#Lower_Case_Token_Filter, and "Stop Token Filter":#Stop_Token_Filter.

p. The following are settings that can be set for a @standard@ analyzer type:

|_. Setting |_. Description |
|stopwords|A list of stopword to initialize the stop filter with. Defaults to the english stop words.|
|max_token_length|The maximum token length. If a token is seen that exceeds this length then it is discarded. Defaults to @255@.|

h2. Simple Analyzer

p. An analyzer of type @simple@ that is built using a "Lower Case Tokenizer":#Lower_Case_Tokenizer.

h2. Stop Analyzer

p. An analyzer of type @stop@ that is built using a "Lower Case Tokenizer":#Lower_Case_Tokenizer, with "Stop Token Filter":#Stop_Token_Filter.

p. The following are settings that can be set for a @stop@ analyzer type:

|_. Setting |_. Description |
|stopwords|A list of stopword to initialize the stop filter with. Defaults to the english stop words.|

h2. Whitespace Analyzer

p. An analyzer of type @whitespace@ that is built using a "Whitespace Tokenizer":#Whitespace_Tokenizer.

h2. Keyword Analyzer

p. An analyzer of type @keyword@ that "tokenizes" an entire stream as a single token. This is useful for data like zip codes, ids and so on. Note, when using mapping definitions, it make more sense to simply mark the field as @not_analyzed@.

h2. Custom Analyzer

p. An analyzer of type @custom@ that allows to combine a @Tokenizer@ with zero or more @Token Filters@. The custom analyzer accepts a logical/registered name of the tokenizer to use, and a list of logical/registered names of token filters.

p. The following are settings that can be set for a @custom@ analyzer type:

|_. Setting |_. Description |
|tokenizer|The logical / registered name of the tokenizer to use.|
|filter|An optional list of logical / registered name of token filters.|

p. Here is an example:

<pre class="prettyprint">
index :
    analysis :
        analyzer : 
            myAnalyzer2 :
                type : custom
                tokenizer : myTokenizer1
                filter : [myTokenFilter1, myTokenFilter2]
        tokenizer :
            myTokenizer1 :
                type : standard
                max_token_length : 900
        filter :
            myTokenFilter1 :
                type : stop
                stopwords : [stop1, stop2, stop3, stop4]
            myTokenFilter2 :
                type : length
                min : 0
                max : 2000
</pre>


h2. Built in Tokenizers

p. If not explicitly defined (for example, by configuring a tokenizer with the same logical name), the following tokenizers are automatically registered (under their respective logical names) and available for use:

|_. Tokenizer Logical Name |_. Description |
|standard|A "Standard Tokenizer":#Standard_Tokenizer registered with default settings.|
|keyword|A "Keyword Tokenizer":#Keyword_Tokenizer registered with default settings.|
|letter|A "Letter Tokenizer":#Letter_Tokenizer registered with default settings.|
|lowercase|A "Lower Case Tokenizer":#Lower_Case_Tokenizer registered with default settings.|
|whitespace|A "Whitespace Tokenizer":#Whitespace_Tokenizer registered with default settings.|
|nGram|A "NGram Tokenizer":#NGram_Tokenizer registered with default settings.|
|edgeNGram|A "EdgeNGram Tokenizer":#EdgeNGram_Tokenizer registered with default settings.|

h2. Standard Tokenizer

p. A tokenizer of type @standard@ providing grammar based tokenizer that is a good tokenizer for most European language documents. It splits words at punctuation characters, removing punctuation. However, a  dot that's not followed by whitespace is considered part of a token. It also splits words at hyphens, unless there's a number in the token, in which case the whole token is interpreted as a product number and is not split. It recognizes email addresses and internet hostnames as one token.

p. The following are settings that can be set for a @standard@ tokenizer type:

|_. Setting |_. Description |
|max_token_length|The maximum token length. If a token is seen that exceeds this length then it is discarded. Defaults to @255@.|

h2. Keyword Tokenizer

p. A tokenizer of type @keyoword@ that emits the entire input as a single input.

p. The following are settings that can be set for a @keyword@ tokenizer type:

|_. Setting |_. Description |
|buffer_size|The term buffer size. Defaults to @256@.|

h2. Letter Tokenizer

p. A tokenizer of type @letter@ that divides text at non-letters.  That's to say, it defines tokens as maximal strings of adjacent letters. Note, this does a decent job for most European languages, but does a terrible job for some Asian languages, where words are not separated by spaces.

h2. Lower Case Tokenizer

p. A tokenizer of type @lowercase@ that performs the function of "Letter Tokenizer":#Letter_Tokenizer and "Lower Case Token Filter":#Lower_Case_Token_Filter together.It divides text at non-letters and converts them to lower case.  While it is functionally equivalent to the combination of "Letter Tokenizer":#Letter_Tokenizer and "Lower Case Token Filter":#Lower_Case_Token_Filter, there is a performance advantage to doing the two tasks at once, hence this (redundant) implementation.

h2. Whitespace Tokenizer

p. A tokenizer of type @whitespace@ that divides text at whitespace.

h2. NGram Tokenizer

p. A tokenizer of type @nGram@.

p. The following are settings that can be set for a @nGram@ tokenizer type:

|_. Setting |_. Description |
|min_gram|Defaults to @1@.|
|max_gram|Defaults to @2@.|

h2. EdgeNGram Tokenizer

p. A tokenizer of type @edgeNGram@.

p. The following are settings that can be set for a @edgeNGram@ tokenizer type:

|_. Setting |_. Description |
|min_gram|Defaults to @1@.|
|max_gram|Defaults to @2@.|
|side|Either @front@ or @back@.|

h2. Built in Token Filters

p. If not explicitly defined (for example, by configuring a token filter with the same logical name), the following token filters are automatically registered (under their respective logical names) and available for use:

|_. Token Filter Logical Name |_. Description |
|stop|A "Stop Token Filter":#Stop_Token_Filter registered with default settings.|
|asciifolding|A "ASCII Folding Token Filter":#ASCII_Folding_Token_Filter registered with default settings.|
|length|A "Length Token Filter":#Length_Token_Filter registered with default settings.|
|lowercase|A "Lower Case Token Filter":#Lower_Case_Token_Filter registered with default settings.|
|porterstem|A "Porter Stem Token Filter":#Porter_Stem_Token_Filter registered with default settings.|
|standard|A "Standard Token Filter":#Standard_Token_Filter registered with default settings.|
|nGram|A "NGram Token Filter":#NGram_Token_Filter registered with default settings.|
|edgeNGram|A "EdgeNGram Token Filter":#EdgeNGram_Token_Filter registered with default settings.|
|shingle|A "Shingle Token Filter":#Shingle_Token_Filter registered with default settings.|

h2. Stop Token Filter

p. A token filter of type @stop@ that removes stop words from token streams.

p. The following are settings that can be set for a @stop@ token filter type:

|_. Setting |_. Description |
|stopwords|A list of stop words to use. Defaults to english stop words.|
|enable_position_increments|Set to @true@ if token positions should record the removed stop words, @false@ otherwise. Defaults to @true@.|
|ignore_case|Set to @true@ to lower case all words first. Defaults to @false@.|

h2. ASCII Folding Token Filter

p. A token filter of type @asciifolding@ that converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127 ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if one exists.

h2. Length Token Filter

p. A token filter of type @length@ that removes words that are too long or too short for the stream.

p. The following are settings that can be set for a @length@ token filter type:

|_. Setting |_. Description |
|min|The minimum number. Defaults to @0@.|
|min|The maximum number. Defaults to @Integer.MAX_VALUE@.|

h2. Lower Case Token Filter

p. A token filter of type @lowercase@ that normalizes token text to lower case.

h2. Porter Stem Token Filter

p. A token filter of type @porterStem@ that transforms the token stream as per the Porter stemming algorithm. 

p. Note, the input to the stemming filter must already be in lower case, so you will need to use "Lower Case Token Filter":#Lower_Case_Token_Filter or "Lower Case Tokenizer"#Lower_Case_Token_Filter farther down the Tokenizer chain in order for this to work properly!. For example, when using custom analyzer, make sure the @lowercase@ filter comes before the @porterStem@ filter in the list of filters.

h2. Standard Token Filter

p. A token filter of type @standard@ that normalizes tokens extracted with the "Standard Tokenizer":#Standard_Tokenizer.

h2. NGram Token Filter

p. A token filter of type @nGram@.

p. The following are settings that can be set for a @nGram@ token filter type:

|_. Setting |_. Description |
|min_gram|Defaults to @1@.|
|max_gram|Defaults to @2@.|

h2. EdgeNGram Token Filter

p. A token filter of type @edgeNGram@.

p. The following are settings that can be set for a @edgeNGram@ token filter type:

|_. Setting |_. Description |
|min_gram|Defaults to @1@.|
|max_gram|Defaults to @2@.|
|side|Either @front@ or @back@.|

h2. Shingle Token Filter

p. A token filter of type @shingle@.

p. The following are settings that can be set for a @shingle@ token filter type:

|_. Setting |_. Description |
|max_shingle_size|Defaults to @2@.|
|output_unigrams|Defaults to @true@.|
